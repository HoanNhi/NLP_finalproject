{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-22T01:11:03.362848Z",
     "start_time": "2025-03-22T01:10:57.159487Z"
    }
   },
   "source": [
    "import fitz  #import PyMuPDF module\n",
    "import os #module used to interact with local Operating System.\n",
    "import pytesseract\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "os.environ['HF_HOME'] = '/mnt/data/thomas/.cache'\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/opt/nvidia/hpc_sdk/Linux_x86_64/24.5/math_libs/12.4/targets/x86_64-linux/lib/\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prompt preparation",
   "id": "95551271dd0841bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Stage 1",
   "id": "763e97d7ca4e6e28"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Text-gen prompts",
   "id": "a60c70fe4b5f2b56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T01:14:32.329092Z",
     "start_time": "2025-03-22T01:14:32.318604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompts_1 = []\n",
    "system_role = {\"role\": \"system\", \"content\": \"You are a helpful academic assistant trained to answer questions about academic policy concisely.\"}\n",
    "prompts_1.append({\"role\": \"user\", \"content\": \"Does withdrawal from a course after 4th week result in a 4 credits deduction in the total credits covered by financial aid?\"})\n",
    "prompts_1.append({\"role\": \"user\", \"content\": \"What are the consequences if I withdraw from capstone II?\"})\n",
    "prompts_1.append({\"role\": \"user\", \"content\": \"Can I continue my study for more than 4 years with financial aid?\"})\n",
    "prompts_1.append({\"role\": \"user\", \"content\": \"Can you differentiate a cross-listed course and elective applied course in the case of a double major?\"})\n",
    "prompts_1.append({\"role\": \"user\", \"content\": \"I would like to ask for the capstone withdrawal policy. What will be the impact when I choose to drop the capstone before the Fall 2025 term starts? Will there be any penalties associated with withdrawing, such as a 'W' notation on my transcript?\"})\n",
    "prompts_1.append({\"role\": \"user\", \"content\": \"I have taken Microeconomic Analysis and Macroeconomic Analysis in Fall 2023. Am I eligible to take Financial Economics in Spring 2024?\"})"
   ],
   "id": "c4c8e5a680fa5a57",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Stage 2",
   "id": "cca0ab3e36438825"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T01:11:03.591408Z",
     "start_time": "2025-03-22T01:11:03.583857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompts_2 = []\n",
    "prompts_2.append({'role': 'user', 'content': 'Context: There are earned credits and attempted credits. Earned credits refer to the number of credit hours a student successfully completes with a grade of D or higher for letter grade options and a Pass for Pass/No Pass options. Attempted credits are the credit hours students enroll in and are recorded in the transcript, regardless of whether they complete the courses or pass them. This includes all courses for which a student receives a grade, including withdrawals (W), incompletes (I), repeats (RP), and failures (F). See section 4 for more details on different types of grades. Question: Does withdrawal from a course after 4th week result in a 4 credits deduction in the total credits covered by financial aid?'})\n",
    "prompts_2.append({'role': 'user', 'content': 'Context: All majors at Fulbright offer an optional capstone pathway to be completed during the student’s final year. Students may apply for the opportunity to fulfill 8 credits of their major requirements via a capstone project. A student wishing to do a capstone must apply during the term prior to the first term of the capstone (Capstone I), and may only apply after having completed 80 credits of coursework. Question: What are the consequences if I withdraw from capstone II?'})\n",
    "prompts_2.append({'role': 'user', 'content': 'Context: Fulbright follows MOET regulations, which state that the maximum time to complete the program cannot exceed double the standard duration. Question: Can I continue my study for more than 4 years with financial aid?'})\n",
    "prompts_2.append({'role': 'user', 'content': 'Context: For Applied Mathematics, Elective Applied Courses (2 required). Students pursuing a double major must complete all requirements for each major. If two majors (or two minors, or a major and a minor) share cross- listed or tagged1 courses, up to 8 credits (two courses) can be used to satisfy both sets of requirements. Question: Can you differentiate a cross-listed course and elective applied course in the case of a double major?'})\n",
    "prompts_2.append({'role': 'user', 'content': \"\"\"Context: All majors at Fulbright offer an optional capstone pathway to be completed during the student’s final year. Students may apply for the opportunity to fulfill 8 credits of their major requirements via a capstone project. A student wishing to do a capstone must apply during the term prior to the first term of the capstone (Capstone I), and may only apply after having completed 80 credits of coursework. Students who do not do a capstone will need to fulfill 8 credits of coursework as specified in their major. Individual majors will determine eligibility requirements, and students should discuss their proposals with program coordinators and potential advisors to prepare their applications. Capstone projects will be graded No Pass/Pass/Honors and must be evaluated by the supervising faculty at least one month before the end of the student’s graduating term. Students whose capstone is graded Honors will be awarded Graduation with Honors, answer this question: I would like to ask for the capstone withdrawal policy. What will be the impact when I choose to drop the capstone before the Fall 2025 term starts? Will there be any penalties associated with withdrawing, such as a 'W' notation on my transcript?\"\"\"})\n",
    "prompts_2.append({'role': 'user', 'content': 'Context: Econometrics: This course is concerned with the application of statistical theory to the analysis of economic data and the estimation of economic relationships. The course focuses on regression analysis and its uses in empirical economic research. Students will learn how to construct economic models and test them with data. Microeconomic Analysis: This course focuses on how incentives both constrain and direct the decision making of consumers, producers, and governments. Students will learn to use both graphical and optimization techniques to solve the problems faced by consumers (what to buy), producers (what to produce and what price to sell it at), and governments (which policies to enact). Macroeconomic Analysis: In this course, students will combine empirical observations and economic models to study the dynamics of the aggregate economy. This course focuses on the macroeconomic tools of government – fiscal and monetary policy – and their effects on long-run economic growth, employment, and inflation. Question: I have taken Microeconomic Analysis and Macroeconomic Analysis in Fall 2023. Am I eligible to take Financial Economics in Spring 2024?'})"
   ],
   "id": "485d0a67b4623f85",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Stage 3",
   "id": "18802224c3cb37fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T01:11:03.771679Z",
     "start_time": "2025-03-22T01:11:03.753912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompts_3 = []\n",
    "prompts_3.append({'role': 'user', 'content': 'Context: There are earned credits and attempted credits. Earned credits refer to the number of credit hours a student successfully completes with a grade of D or higher for letter grade options and a Pass for Pass/No Pass options. Attempted credits are the credit hours students enroll in and are recorded in the transcript, regardless of whether they complete the courses or pass them. This includes all courses for which a student receives a grade, including withdrawals (W), incompletes (I), repeats (RP), and failures (F). See section 4 for more details on different types of grades. There are two Withdrawal periods per term. The first period starts after 4:00 PM Friday, Week 2 to 4:00 PM Friday, Week 4. During this time, students can request to withdraw from the course without any consequence. The second period starts after 4:00 PM Friday, Week 4 to 4:00 PM Friday, Week 8. Attempted credits and a \"W\" grade will be recorded on the transcript for the course(s) from which students withdraw in this period. This grade does not affect a student’s GPA. In order to withdraw from a course, students need to submit the Course Withdrawal form to the Registrar’s Office, following the above timeline. A student who withdraws from a course beyond the halfway point of that course (end of week 4 for an 8-week course or end of week 8 for a full-term course) automatically receives a grade of ‘F’, which will affect the student’s GPA. Exceptions will only be permitted in exceptional circumstances (see Section 12), and it requires supporting documentation as well as the authorization of course instructor(s), the student’s academic/major advisor, and the Registrar’s Office. Question: Does withdrawal from a course after 4th week result in a 4 credits deduction in the total credits covered by financial aid?'})\n",
    "\n",
    "prompts_3.append({'role': 'user', 'content': 'Context: All majors at Fulbright offer an optional capstone pathway to be completed during the student’s final year. Students may apply for the opportunity to fulfill 8 credits of their major requirements via a capstone project. A student wishing to do a capstone must apply during the term prior to the first term of the capstone (Capstone I), and may only apply after having completed 80 credits of coursework. Students who do not do a capstone will need to fulfill 8 credits of coursework as specified in their major. Individual majors will determine eligibility requirements, and students should discuss their proposals with program coordinators and potential advisors to prepare their applications. Capstone projects will be graded No Pass/Pass/Honors and must be evaluated by the supervising faculty at least one month before the end of the student’s graduating term. Students whose capstone is graded Honors will be awarded Graduation with Honors. Question: What are the consequences if I withdraw from capstone II?'})\n",
    "\n",
    "prompts_3.append({'role': 'user', 'content': 'Context: Fulbright follows MOET regulations, which state that the maximum time to complete the program cannot exceed double the standard duration. In other words, students have a maximum of 8 years to complete their studies. Question: Can I continue my study for more than 4 years with financial aid?'})\n",
    "\n",
    "prompts_3.append({'role': 'user', 'content': 'Context: For Applied Mathematics, These are the courses from other majors that emphasize the applications of Mathematics.  Students will have to take at least 2 applied courses (at least one at 200 level). If there is any prerequisite, students need to either complete the course’s prerequisites or obtain the instructor’s approval. Students pursuing a double major must complete all requirements for each major. If two majors (or two minors, or a major and a minor) share cross-listed or tagged courses, up to 8 credits (two courses) can be used to satisfy both sets of requirements. This limit is per each pair of majors (or major and minor) and does not restrict the total number of double-counted credits or courses a student can claim. For example, a student can major in both A and B and minor in C with a total of 6 double-counted courses: two in A and B, two in B and C, and two in A and C. This rule does not apply to courses that are not cross-listed or tagged. For instance, if major A requires certain courses from major B for out-of-area foundation, exploration, or applications of the major, and these courses are not cross-listed between A and B, they do not count toward the 8-credit limit. Question: Can you differentiate a cross-listed course and elective applied course in the case of a double major?'})\n",
    "\n",
    "\n",
    "prompts_3.append({'role': 'user', 'content': \"\"\"Context: All majors at Fulbright offer an optional capstone pathway to be completed during the student’s final year. Students may apply for the opportunity to fulfill 8 credits of their major requirements via a capstone project. A student wishing to do a capstone must apply during the term prior to the first term of the capstone (Capstone I), and may only apply after having completed 80 credits of coursework. Students who do not do a capstone will need to fulfill 8 credits of coursework as specified in their major. Individual majors will determine eligibility requirements, and students should discuss their proposals with program coordinators and potential advisors to prepare their applications. Capstone projects will be graded No Pass/Pass/Honors and must be evaluated by the supervising faculty at least one month before the end of the student’s graduating term. Students whose capstone is graded Honors will be awarded Graduation with Honors. Attempted credits and a \"W\" grade will be recorded on the transcript for the course(s) from which students withdraw in this period. This grade does not affect a student’s GPA. In order to withdraw from a course, students need to submit the Course Withdrawal form to the Registrar’s Office, following the above timeline. A student who withdraws from a course beyond the halfway point of that course (end of week 4 for an 8-week course or end of week 8 for a full-term course) automatically receives a grade of ‘F’, which will affect the student’s GPA. Question: I would like to ask for the capstone withdrawal policy. What will be the impact when I choose to drop the capstone before the Fall 2025 term starts? Will there be any penalties associated with withdrawing, such as a 'W' notation on my transcript?\"\"\"})\n",
    "\n",
    "prompts_3.append({'role': 'user', 'content': 'Context: Econometrics: This course is concerned with the application of statistical theory to the analysis of economic data and the estimation of economic relationships. The course focuses on regression analysis and its uses in empirical economic research. Students will learn how to construct economic models and test them with data. Microeconomic Analysis: This course focuses on how incentives both constrain and direct the decision making of consumers, producers, and governments. Students will learn to use both graphical and optimization techniques to solve the problems faced by consumers (what to buy), producers (what to produce and what price to sell it at), and governments (which policies to enact). Macroeconomic Analysis: In this course, students will combine empirical observations and economic models to study the dynamics of the aggregate economy. This course focuses on the macroeconomic tools of government – fiscal and monetary policy – and their effects on long-run economic growth, employment, and inflation. Advanced level courses in Economics can be taken after Microeconomic and Macroeconomic Analysis and Econometrics. Financial Economics is counted as an advanced course. Question: I have taken Microeconomic Analysis and Macroeconomic Analysis in Fall 2023. Am I eligible to take Financial Economics in Spring 2024?'})"
   ],
   "id": "7bfd8724fa4f3dbf",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Qwen model",
   "id": "6c64ec4375a72f4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Loading",
   "id": "c8db459f7d57227e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T22:55:27.303060Z",
     "start_time": "2025-03-21T22:55:09.288827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_id_qwen_25_7B = \"Qwen/Qwen2.5-7B-Instruct-1M\"\n",
    "\n",
    "pipeline_qwen_25_72B_text_gen = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id_qwen_25_7B,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")"
   ],
   "id": "d35f28f2a9c3d0f4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5761e87b1cbc4e1c938101158ca1878c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Question-answering",
   "id": "135c49a7306db191"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T23:00:52.889208Z",
     "start_time": "2025-03-21T22:59:19.198231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Qwene 7B-1M\n",
    "for question in prompts_1:\n",
    "    questions = []\n",
    "    questions.append(system_role)\n",
    "    questions.append(question)\n",
    "    outputs = pipeline_qwen_25_72B_text_gen(questions, max_new_tokens = 1024, num_beams = 2)\n",
    "    print(\"The answer by Qwen 2.5 with 7B is: \")\n",
    "    print(outputs[0]['generated_text'][-1])\n",
    "    print(\"-\"*100)"
   ],
   "id": "44abd8737af97fd9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer by Qwen 2.5 with 7B is: \n",
      "{'role': 'assistant', 'content': \"Withdrawal from a course after the fourth week typically does not result in a direct deduction of credits from the total covered by financial aid. However, it can affect your financial aid eligibility depending on your school's specific policies regarding the Return to Title IV (R2T4) policy.\\n\\nThe R2T4 policy calculates the amount of financial aid that you have earned based on the percentage of the term you completed. If you withdraw after the fourth week, you may be considered to have completed a certain percentage of the term, and the amount of aid you are eligible to keep is recalculated. This could potentially lead to a reduction in the amount of aid you receive for the term, but it does not directly deduct credits from your total covered by financial aid.\\n\\nFor precise information, you should consult your institution's financial aid office or review your school's specific R2T4 policy.\"}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Qwen 2.5 with 7B is: \n",
      "{'role': 'assistant', 'content': 'The consequences of withdrawing from Capstone II can vary depending on your institution\\'s academic policies. Generally, withdrawing from a capstone course may result in the following:\\n\\n1. **Grade Impact**: You will receive a grade of \"W\" (Withdrawal) on your transcript, which does not count as a grade point but can affect your overall grade point average if you take the course again.\\n\\n2. **Credit Loss**: You will lose the credit hours associated with the course, which may impact your progress toward graduation.\\n\\n3. **Financial Aid**: If you are receiving financial aid, withdrawing from a course can affect your eligibility for aid under the Satisfactory Academic Progress (SAP) standards.\\n\\n4. **Timeline**: It may affect your timeline to complete your degree, as you will need to retake the course.\\n\\n5. **Special Requirements**: Some programs may have specific requirements for completing capstone courses, and withdrawal might require you to meet those requirements in a different way.\\n\\nIt\\'s important to consult your academic advisor or the registrar\\'s office at your institution to understand the specific consequences for your situation.'}\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer by Qwen 2.5 with 7B is: \n",
      "{'role': 'assistant', 'content': 'The ability to continue your studies for more than four years with financial aid depends on several factors, including the type of financial aid you are receiving, your academic progress, and the policies of your institution. Here are some general points to consider:\\n\\n1. **Federal Financial Aid**: Federal financial aid, such as Pell Grants and Direct Loans, is typically designed for students to complete their degree within a standard time frame, which is usually six years for a bachelor’s degree. However, some students may be eligible for extensions under certain circumstances.\\n\\n2. **Institutional Aid**: Check with your institution’s financial aid office for specific policies regarding extended enrollment. Some schools may offer additional aid for students who need more time to complete their degree.\\n\\n3. **Academic Standing**: Maintaining good academic standing is crucial. If you are not making satisfactory academic progress, you may lose eligibility for financial aid.\\n\\n4. **Special Circumstances**: If you have extenuating circumstances (e.g., health issues, family emergencies), you may be eligible for an appeal to the financial aid office to request additional time.\\n\\n5. **Private Scholarships and Grants**: Some private scholarships and grants may have different terms and conditions regarding the duration of study.\\n\\nIt’s important to consult with your financial aid advisor or the financial aid office at your institution to understand the specific rules and options available to you.'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Qwen 2.5 with 7B is: \n",
      "{'role': 'assistant', 'content': 'Certainly! In the context of a double major, the terms \"cross-listed course\" and \"elective applied course\" refer to different types of courses that can be part of your academic program:\\n\\n1. **Cross-listed Course**: \\n   - A cross-listed course is one that is offered by two or more departments and can be counted toward the requirements of more than one major or minor. For example, a course might be cross-listed between the Department of Economics and the Department of Political Science, allowing students in either major to take it.\\n   - In the case of a double major, a cross-listed course can fulfill requirements for both majors, which can be particularly useful if there is overlap in the curriculum of the two majors.\\n\\n2. **Elective Applied Course**:\\n   - An elective applied course is a course that is not required for either major but can be chosen by the student to fulfill elective credit requirements. These courses are typically applied in nature, meaning they have a practical or real-world application, and they can be in any department or field that interests the student.\\n   - In a double major scenario, an elective applied course would be one that is not part of the core or required courses for either major but can still count towards the overall degree requirements, including electives.\\n\\nIn summary, a cross-listed course can fulfill requirements for multiple majors, while an elective applied course is a choice made by the student to meet elective credit needs, often with a practical focus.'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Qwen 2.5 with 7B is: \n",
      "{'role': 'assistant', 'content': \"The capstone withdrawal policy can vary significantly between institutions, so it's important to refer to your specific institution's academic policies for precise details. However, I can provide a general overview based on common practices:\\n\\n1. **Impact of Dropping Before the Fall 2025 Term**: If you withdraw from the capstone before the Fall 2025 term starts, the impact will depend on your institution's policy. Typically, withdrawing before the start of the term might not result in a 'W' notation on your transcript, but you should check your institution's specific policy.\\n\\n2. **Penalties**: Generally, there are no penalties associated with withdrawing from a capstone course before the start of the term, such as a 'W' notation on your transcript. However, if you withdraw after the start of the term, you might receive a 'W' notation, which does not affect your GPA but can impact your academic standing or financial aid eligibility.\\n\\nTo get the most accurate and up-to-date information, you should consult your institution's academic catalog or speak directly with an academic advisor. They can provide you with the specific details and implications of withdrawing from the capstone course before the Fall 2025 term.\"}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Qwen 2.5 with 7B is: \n",
      "{'role': 'assistant', 'content': \"Yes, you are generally eligible to take Financial Economics in Spring 2024 after completing Microeconomic Analysis and Macroeconomic Analysis in Fall 2023. These prerequisites are commonly required for a Financial Economics course as they provide foundational knowledge in microeconomics and macroeconomics that is essential for understanding financial economics. However, it's always a good idea to check the specific course syllabus or consult with the instructor or academic advisor to confirm the prerequisites for the Financial Economics course you plan to take.\"}\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T23:02:08.978735Z",
     "start_time": "2025-03-21T23:00:52.965847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Qwene 7B-1M\n",
    "for question in prompts_2:\n",
    "    questions = []\n",
    "    questions.append(system_role)\n",
    "    questions.append(question)\n",
    "    outputs = pipeline_qwen_25_72B_text_gen(questions, max_new_tokens = 1024, num_beams = 2)\n",
    "    print(\"The answer by Qwen 2.5 with 7B is: \")\n",
    "    print(outputs[0]['generated_text'][-1])\n",
    "    print(\"-\"*100)"
   ],
   "id": "bdf989bc8306dce7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer by Qwen 2.5 with 7B is: \n",
      "{'role': 'assistant', 'content': 'Withdrawals from a course after the fourth week typically do not result in a direct deduction of credits from the total covered by financial aid, but they can affect the calculation of satisfactory academic progress (SAP) and eligibility for financial aid in other ways. \\n\\nFinancial aid institutions generally use the \"aid eligibility\" period, which is often the first 60% of the semester, to determine how much aid a student is eligible for. If a student withdraws after the fourth week, they may be considered to have completed a portion of the semester, and the amount of aid they are eligible for may be prorated based on the percentage of the semester they attended.\\n\\nFor specific policies, you should refer to the financial aid section of your institution\\'s policy manual or contact the financial aid office directly, as policies can vary between institutions.'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Qwen 2.5 with 7B is: \n",
      "{'role': 'assistant', 'content': 'If you withdraw from Capstone II, the specific consequences can vary depending on the policies of Fulbright University. Generally, withdrawing from a capstone course can result in the following:\\n\\n1. **Loss of Credits**: You may lose the 4 credits associated with Capstone II.\\n2. **Impact on Graduation Requirements**: If Capstone II is part of your graduation requirements, withdrawing could delay your graduation.\\n3. **Grading Impact**: You may receive a grade of \"W\" (Withdrawal) on your transcript, which could have implications for your academic record.\\n4. **Financial Implications**: There may be financial consequences, such as the need to pay for the course again if you wish to retake it.\\n\\nIt\\'s important to review the university\\'s academic policies or consult with an academic advisor to understand the specific consequences of withdrawing from Capstone II at Fulbright University.'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Qwen 2.5 with 7B is: \n",
      "{'role': 'assistant', 'content': 'According to the MOET regulations followed by Fulbright, the maximum time to complete the program cannot exceed double the standard duration. If the standard duration of the program is 2 years, then the maximum duration with financial aid is 4 years. Therefore, under normal circumstances, you cannot continue your study for more than 4 years with Fulbright financial aid based on the MOET regulations. If you have specific circumstances or need to extend your stay, you would need to seek approval from Fulbright authorities, but this would likely require justification beyond the standard program duration.'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Qwen 2.5 with 7B is: \n",
      "{'role': 'assistant', 'content': 'Certainly! In the context of a double major, the distinction between a cross-listed course and an elective applied course is as follows:\\n\\n1. **Cross-Listed Course**: This is a course that is listed under more than one department or program. For example, a course might be listed under both the Mathematics department and the Engineering department. If two majors share cross-listed courses, up to 8 credits (two courses) can be used to satisfy the requirements for both majors. This means that if a student is majoring in Applied Mathematics and another field that shares a cross-listed course, they can use that course to fulfill the requirements for both majors, up to a maximum of 8 credits.\\n\\n2. **Elective Applied Course**: These are courses that are specifically designated as electives within the Applied Mathematics major. Elective applied courses are chosen by the student from a list of approved courses that are relevant to the field of Applied Mathematics. Elective applied courses do not have the cross-listing feature that allows them to be used to fulfill requirements for another major. Each major typically has its own set of elective courses that must be completed separately.\\n\\nIn summary, a cross-listed course can be shared between majors and used to fulfill requirements for both, up to 8 credits, while an elective applied course is specific to the Applied Mathematics major and cannot be used to fulfill requirements for another major unless it is cross-listed.'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Qwen 2.5 with 7B is: \n",
      "{'role': 'assistant', 'content': \"Based on the information provided about the capstone pathway at Fulbright, there is no specific mention of a capstone withdrawal policy or penalties for dropping a capstone. Typically, academic policies regarding withdrawal from capstone projects are not explicitly detailed in such general descriptions. However, it is common for institutions to have standard withdrawal policies that apply to all courses, which might include a 'W' notation on your transcript.\\n\\nTo get precise information about the capstone withdrawal policy and any potential penalties, you should:\\n\\n1. **Consult the Fulbright Student Handbook or Academic Catalog:** These documents often contain detailed policies on course withdrawal, including capstones.\\n2. **Speak with Your Academic Advisor:** They can provide guidance specific to Fulbright’s policies and procedures.\\n3. **Contact the Program Coordinator:** Since capstone projects are often overseen by specific programs, the coordinator can give you the most accurate and up-to-date information.\\n\\nIn the absence of specific information, it is advisable to assume that withdrawing from a capstone would follow the general course withdrawal policies of Fulbright, which might include a 'W' notation on your transcript.\"}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Qwen 2.5 with 7B is: \n",
      "{'role': 'assistant', 'content': \"Yes, you are likely eligible to take Financial Economics in Spring 2024 after completing Microeconomic Analysis and Macroeconomic Analysis in Fall 2023. Typically, Financial Economics builds upon the foundational knowledge provided by microeconomic and macroeconomic principles. However, specific prerequisites can vary by institution, so it's a good idea to check the course syllabus or consult with an academic advisor to confirm that you meet all requirements for the Financial Economics course.\"}\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T23:03:28.524405Z",
     "start_time": "2025-03-21T23:02:09.032938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Qwene 7B-1M\n",
    "for question in prompts_3:\n",
    "    questions = []\n",
    "    questions.append(system_role)\n",
    "    questions.append(question)\n",
    "    outputs = pipeline_qwen_25_72B_text_gen(questions, max_new_tokens = 1024, num_beams = 2)\n",
    "    print(\"The answer by Qwen 2.5 with 7B is: \")\n",
    "    print(outputs[0]['generated_text'][-1])\n",
    "    print(\"-\"*100)"
   ],
   "id": "e698806ca4cdbcdc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer by Qwen 2.5 with 7B is: \n",
      "{'role': 'assistant', 'content': 'Withdrawal from a course after the 4th week but before the halfway point of the course (end of Week 4 for an 8-week course or end of Week 8 for a full-term course) will result in the course being recorded with a \"W\" grade on the transcript. This \"W\" grade does not affect the student’s GPA. However, the course will still count as attempted credits, and therefore, it will still be covered by financial aid.\\n\\nIf a student withdraws from a course after the halfway point of the course, they will receive an \"F,\" which does affect the GPA and will also be counted as an attempted credit for financial aid purposes.\\n\\nThere is no automatic deduction of 4 credits in the total credits covered by financial aid for withdrawing from a course after the 4th week. The financial aid eligibility is determined based on the total attempted and earned credits, not the timing of the withdrawal itself.'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Qwen 2.5 with 7B is: \n",
      "{'role': 'assistant', 'content': \"If you withdraw from Capstone II, the consequences will depend on the policies of your specific major and the academic regulations of Fulbright University. Generally, withdrawing from Capstone II would mean that you will not receive the 8 credits that the capstone fulfills, and you will need to complete the 8 credits of coursework as specified in your major requirements instead. \\n\\nAdditionally, if you have already been awarded Graduation with Honors based on your capstone being graded Honors, withdrawing from Capstone II would result in the revocation of that distinction. It's important to discuss your situation with your program coordinator or academic advisor to understand the specific implications for your academic record and degree requirements.\"}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Qwen 2.5 with 7B is: \n",
      "{'role': 'assistant', 'content': 'Yes, you can continue your studies for more than 4 years with financial aid, but you must ensure that you do not exceed the maximum time frame allowed by MOET regulations, which is 8 years for the Fulbright program. This means you could potentially continue your studies for up to 4 additional years beyond the initial 4 years, as long as you remain eligible for financial aid and meet all other program requirements.'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Qwen 2.5 with 7B is: \n",
      "{'role': 'assistant', 'content': 'Certainly! Here\\'s a differentiation between a cross-listed course and an elective applied course in the context of a double major:\\n\\n1. **Cross-Listed Course:**\\n   - **Definition:** A cross-listed course is one that is officially listed under the curriculum of two or more departments or majors. This means it can be counted towards the requirements of multiple majors or minors.\\n   - **Example:** If a course is cross-listed between Applied Mathematics and Physics, it can be used to fulfill requirements for both majors.\\n   - **Double Counting:** Up to 8 credits (two courses) can be used to satisfy both sets of requirements for each pair of majors (or major and minor). This means if you are double majoring in Applied Mathematics and Physics, you can use up to 8 credits from cross-listed courses to fulfill the requirements of both majors.\\n\\n2. **Elective Applied Course:**\\n   - **Definition:** An elective applied course is a course that emphasizes the applications of mathematics but is not cross-listed. It is chosen by the student as part of the applied mathematics requirements.\\n   - **Example:** A course like \"Mathematical Modeling in Economics\" might be an elective applied course in Applied Mathematics.\\n   - **Double Counting:** Elective applied courses do not count towards the 8-credit double-counting limit. They are separate from cross-listed courses and are used to fulfill the elective applied course requirement for the major.\\n\\nIn summary, cross-listed courses can be double-counted to satisfy requirements of multiple majors or minors up to the 8-credit limit, while elective applied courses are not included in this limit and are used to fulfill the elective applied course requirement for the major.'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Qwen 2.5 with 7B is: \n",
      "{'role': 'assistant', 'content': \"Based on the information provided in the context, if you choose to drop the capstone project before the Fall 2025 term starts, you will not receive a 'W' notation on your transcript. The policy specifically states that a 'W' grade will be recorded on the transcript for the course(s) from which students withdraw in the period following the application deadline for the capstone (Capstone I) and before the end of the student’s graduating term. Since you are dropping the capstone before the Fall 2025 term starts, this period does not apply to you.\\n\\nAdditionally, there are no penalties mentioned for withdrawing from the capstone project before the application deadline for Capstone I. However, it is important to note that if you do not complete the capstone, you will need to fulfill the 8 credits of coursework as specified in your major instead.\\n\\nMake sure to discuss this decision with your program coordinator and potential advisors to ensure you are aware of any other implications or requirements specific to your major.\"}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Qwen 2.5 with 7B is: \n",
      "{'role': 'assistant', 'content': 'Based on the information provided, you are eligible to take Financial Economics in Spring 2024. The context states that advanced level courses in Economics, including Financial Economics, can be taken after completing Microeconomic and Macroeconomic Analysis. Since you have already completed these two courses in Fall 2023, you meet the prerequisite requirements to enroll in Financial Economics in Spring 2024.'}\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T23:04:14.957891Z",
     "start_time": "2025-03-21T23:04:14.951942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# model_qwen_25_72B = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "# pipeline_qwen_25_72B = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model_qwen_25_72B,\n",
    "#     model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "#     device_map=\"auto\",\n",
    "# )"
   ],
   "id": "16670748734669fe",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# pipeline_qwen_25_72B_QA = transformers.pipeline(\n",
    "#     \"question-answering\",\n",
    "#     model=\"distilbert/distilbert-base-cased-distilled-squad\",\n",
    "#     model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "#     device_map=\"cpu\",\n",
    "# )"
   ],
   "id": "63544ced482d226"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Llama-3.1-Tulu-3-8B",
   "id": "90a5c57f03b7ad82"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model loading",
   "id": "78fb8996568e4dda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T23:05:43.907759Z",
     "start_time": "2025-03-21T23:05:43.901821Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.empty_cache()",
   "id": "8aab779199d84345",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T23:05:55.403703Z",
     "start_time": "2025-03-21T23:05:45.415740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_llama_31_tulu_8B = \"allenai/Llama-3.1-Tulu-3-8B\"\n",
    "pipeline_llama_31_tulu_8B = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_llama_31_tulu_8B,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")"
   ],
   "id": "6ffac2ec989c8591",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c973496c64ef42ae895d593727392b4f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Question-answering",
   "id": "686966df06446ead"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T23:30:48.765750Z",
     "start_time": "2025-03-21T23:06:41.776103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Qwene 7B-1M\n",
    "for question in prompts_1:\n",
    "    questions = []\n",
    "    questions.append(system_role)\n",
    "    questions.append(question)\n",
    "    outputs = pipeline_llama_31_tulu_8B(questions, max_new_tokens = 1024, num_beams = 2)\n",
    "    print(\"The answer by Llama 3.1 Tulu 8B is: \")\n",
    "    print(outputs[0]['generated_text'][-1])\n",
    "    print(\"-\"*100)"
   ],
   "id": "1465961ed99770f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer by Llama 3.1 Tulu 8B is: \n",
      "{'role': 'assistant', 'content': \"In many academic institutions, withdrawing from a course after a specific deadline, often the 4th week of the term, can indeed impact financial aid. However, the exact policy can vary significantly from one institution to another. Generally, withdrawing from a course after this point may result in a reduction in the total credits covered by financial aid, potentially leading to a decrease in the amount of financial aid received. This is because financial aid is typically calculated based on the number of credits a student is enrolled in at the time of disbursement.\\n\\nHere are some key points to consider:\\n\\n1. **Institutional Policy**: Check the specific policy of your institution regarding withdrawal and financial aid. Some schools may have a sliding scale where the more credits you withdraw from, the greater the impact on your financial aid.\\n\\n2. **Satisfactory Academic Progress (SAP)**: Withdrawing from courses can also affect your SAP status, which is a requirement for maintaining eligibility for federal financial aid.\\n\\n3. **Refund Policy**: Understand the university's refund policy for tuition and fees after the drop/withdrawal deadline. This can also influence how financial aid is adjusted.\\n\\n4. **Consult Financial Aid Office**: The best course of action is to consult directly with the financial aid office at your institution. They can provide you with the most accurate and relevant information based on your specific situation.\\n\\nAlways remember to review your institution's academic calendar and financial aid handbook for detailed information on withdrawal policies and their implications on financial aid.\"}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Llama 3.1 Tulu 8B is: \n",
      "{'role': 'assistant', 'content': \"If you withdraw from Capstone II, you may face several academic consequences, depending on your institution's specific policies. Generally, these could include:\\n\\n1. **Grade Impact**: You might receive a withdrawal (W) grade, which could affect your GPA. Some institutions treat Ws differently than other grades, so check your institution's policy.\\n\\n2. **Progression Requirements**: Capstone courses are often required for graduation. Withdrawing could delay your graduation timeline.\\n\\n3. **Financial Implications**: You may still be responsible for tuition and fees associated with the course, depending on your institution's refund policy.\\n\\n4. **Repeated Course Policy**: If you need to retake the course, check if there are any restrictions or additional requirements.\\n\\n5. **Academic Standing**: Frequent withdrawals could negatively impact your academic standing or eligibility for financial aid.\\n\\nAlways refer to your institution's academic catalog or consult with your academic advisor for the most accurate and detailed information regarding the consequences of withdrawing from Capstone II.\"}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Llama 3.1 Tulu 8B is: \n",
      "{'role': 'assistant', 'content': \"In most academic institutions, financial aid is typically awarded for a period of up to four years (or eight semesters) for undergraduate students pursuing a bachelor's degree. However, there are several factors to consider:\\n\\n1. **Program Length**: Some programs may require more than the standard four years to complete, especially in fields like engineering, certain sciences, or professional degrees. In such cases, students may continue to receive financial aid as long as they are making satisfactory academic progress (SAP) towards their degree.\\n\\n2. **Satisfactory Academic Progress (SAP)**: To maintain eligibility for financial aid, students must adhere to their institution's SAP policy, which usually includes maintaining a certain GPA and completing a certain percentage of their coursework each semester.\\n\\n3. **Graduate Studies**: If you decide to pursue a graduate degree after completing your undergraduate studies, you may be eligible for different types of financial aid, such as graduate assistantships, fellowships, or loans.\\n\\n4. **Extensions and Appeals**: If you're unable to complete your degree in the standard timeframe due to extenuating circumstances, you may be able to request an extension or file an appeal with your financial aid office to continue receiving aid.\\n\\nIt's important to consult directly with your financial aid office to understand the specific policies and procedures at your institution. They can provide you with detailed information about your eligibility for continued financial aid beyond the standard four-year period.\"}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Llama 3.1 Tulu 8B is: \n",
      "{'role': 'assistant', 'content': \"A cross-listed course is a single course that is offered by two different departments, allowing students to gain credit towards multiple disciplines. An elective applied course, on the other hand, is a specific course chosen by a student from a list of available options to fulfill elective requirements within their major or degree program.\\n\\nIn the context of a double major, cross-listed courses can be particularly useful as they allow students to count a single course towards the requirements of both majors. This can save time and space in the student's schedule.\\n\\nElective applied courses, meanwhile, are chosen to complement the student's major studies and may not necessarily be cross-listed. They provide flexibility for the student to explore additional areas of interest or to deepen their understanding in a related field.\\n\\nIt's important for students to consult with their academic advisors to ensure that the courses they select, whether cross-listed or electives, meet the requirements for both of their majors.\"}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Llama 3.1 Tulu 8B is: \n",
      "{'role': 'assistant', 'content': \"The capstone withdrawal policy typically allows students to withdraw from a capstone course with certain conditions before the start of the term. Generally, the deadline for withdrawal without academic penalty is well before the term begins, often around the add/drop period.\\n\\nImpact and Penalties:\\n- **Before the Deadline:** If you withdraw from the capstone course before the established deadline, there is usually no impact on your academic record, and you will not receive a 'W' notation on your transcript.\\n- **After the Deadline:** If you withdraw after the deadline, you may receive a 'W' on your transcript, indicating that you withdrew from the course. This can have implications for your academic standing, financial aid, and progress toward degree completion.\\n\\nSpecifics:\\n- **Check the Calendar:** Always refer to the academic calendar provided by your institution for the exact withdrawal deadlines.\\n- **Financial Implications:** Be aware that withdrawing from a course may also affect your tuition fees and any financial aid you may have received.\\n- **Academic Standing:** A 'W' may affect your GPA and could impact your eligibility for honors, scholarships, or certain academic programs.\\n- **Consult with Advisors:** It's always a good idea to consult with your academic advisor or the registrar's office to fully understand the implications of withdrawing from a capstone course.\\n\\nFor the Fall 2025 term specifically, you should check the academic calendar provided by your institution for the exact withdrawal deadlines and any associated policies. It's important to make an informed decision based on your academic and personal circumstances.\"}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Llama 3.1 Tulu 8B is: \n",
      "{'role': 'assistant', 'content': \"To determine your eligibility for the Financial Economics course in Spring 2024, you would typically need to check the prerequisites set by the academic institution offering the course. Generally, successful completion of both Microeconomic Analysis and Macroeconomic Analysis courses would be a prerequisite for a Financial Economics course, as it builds upon the foundational economic principles taught in those subjects.\\n\\nHowever, the specific requirements can vary by institution and program. Here's what you should do:\\n\\n1. **Check the Course Syllabus**: Look for the course syllabus or prerequisites listed on the university's course catalog or the department's website.\\n\\n2. **Contact the Department**: Reach out to the department offering the Financial Economics course. They can provide you with the most accurate and up-to-date information regarding the prerequisites.\\n\\n3. **Consult with an Academic Advisor**: Schedule an appointment with your academic advisor to discuss your academic plan and to confirm if you meet the requirements for the course.\\n\\nAssuming the standard prerequisites are in place, having completed both Microeconomic Analysis and Macroeconomic Analysis should make you eligible for the Financial Economics course, provided there are no other specific requirements such as a minimum GPA or additional coursework.\\n\\nRemember to also check if there are any co-requisites or recommended courses that might enhance your learning experience in Financial Economics.\"}\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T23:51:43.065955Z",
     "start_time": "2025-03-21T23:30:48.838958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Qwene 7B-1M\n",
    "for question in prompts_2:\n",
    "    questions = []\n",
    "    questions.append(system_role)\n",
    "    questions.append(question)\n",
    "    outputs = pipeline_llama_31_tulu_8B(questions, max_new_tokens = 1024, num_beams = 2)\n",
    "    print(\"The answer by Llama 3.1 Tulu 8B is: \")\n",
    "    print(outputs[0]['generated_text'][-1])\n",
    "    print(\"-\"*100)"
   ],
   "id": "4ee8df3ae4d933c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer by Llama 3.1 Tulu 8B is: \n",
      "{'role': 'assistant', 'content': \"Answer: No, withdrawing from a course after the 4th week typically does not result in a 4-credit deduction in the total credits covered by financial aid. The impact of a withdrawal on financial aid eligibility depends on the institution's specific policies and the timing of the withdrawal relative to the academic calendar and financial aid disbursement schedule. Generally, federal financial aid regulations allow students to withdraw from courses and still retain eligibility for aid, but institutions may have their own policies regarding the use of financial aid funds after a certain point in the term. It's important for students to consult their institution's financial aid office for precise information on how withdrawals affect their financial aid package. Always refer to the institution's official academic and financial aid policy documents for the most accurate and applicable information.\"}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Llama 3.1 Tulu 8B is: \n",
      "{'role': 'assistant', 'content': \"Answer: If you withdraw from Capstone II, you may face several consequences, depending on the academic policies of Fulbright University. Generally, withdrawing from a course could result in:\\n\\n1. **Grade Impact**: You may receive a 'W' (withdrawal) on your transcript, which could impact your GPA, depending on the university's grading policy.\\n\\n2. **Progression Requirements**: Fulbright may have specific requirements for progression, such as maintaining a certain GPA or completing a certain number of credits. Withdrawing from Capstone II might affect your ability to graduate on time or at all.\\n\\n3. **Capstone Completion**: Since the capstone is an optional pathway to fulfill major requirements, withdrawing from Capstone II could mean you'll need to complete the regular major requirements, which might extend your time to graduation.\\n\\n4. **Financial Aid and Scholarships**: Depending on the terms of your financial aid or scholarships, withdrawing from a course could affect your eligibility or the amount you receive.\\n\\n5. **Reapplication Process**: If the capstone is a prerequisite for certain opportunities (e.g., honors programs, scholarships), withdrawing from Capstone II might require you to reapply for those opportunities.\\n\\nIt's important to review Fulbright University's specific academic policies regarding course withdrawals, as well as consult with your academic advisor to understand the full implications of withdrawing from Capstone II on your academic journey and future plans.\"}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Llama 3.1 Tulu 8B is: \n",
      "{'role': 'assistant', 'content': 'No, according to the regulations set by MOET (Ministry of Education and Training) and followed by the Fulbright Program, you cannot extend your study period beyond double the standard duration of your program, which is typically 2 years. Therefore, you cannot continue your study for more than 4 years with financial aid from the Fulbright Program. It is important to adhere to these regulations to maintain the integrity and structure of the academic program.'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Llama 3.1 Tulu 8B is: \n",
      "{'role': 'assistant', 'content': 'In the context of academic policy, a cross-listed course is a single course that is offered by two different departments or disciplines. This means that the content and credits of the course are recognized by both departments, allowing students to count the course towards the requirements of either major they are pursuing.\\n\\nAn elective applied course, on the other hand, is a specific type of course that is chosen by the student from a list of approved courses to fulfill the elective requirements of a major or minor. These courses are typically more specialized and are intended to provide students with additional skills or knowledge relevant to their field of study.\\n\\nIn the case of a double major, cross-listed courses can be particularly advantageous because they allow students to double-count up to 8 credits (or two courses) towards the requirements of both majors. This means that a student could take a single cross-listed course and apply it to the requirements of both majors, effectively reducing the total number of courses the student needs to complete.\\n\\nElective applied courses, however, are chosen to supplement the core requirements of each major. While they may also be cross-listed, their primary purpose is to provide flexibility for students to tailor their education to their specific interests within their chosen fields.\\n\\nTo summarize:\\n\\n- **Cross-listed course:** A single course offered by two departments, allowing students to count it towards the requirements of either major.\\n- **Elective applied course:** A specialized course chosen by the student to fulfill elective requirements in a major or minor.\\n\\nIn the context of a double major, cross-listed courses can be used strategically to reduce the overall course load, while elective applied courses provide additional opportunities for specialization within each major.'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Llama 3.1 Tulu 8B is: \n",
      "{'role': 'assistant', 'content': \"At Fulbright University, the capstone withdrawal policy typically allows students to withdraw from the capstone pathway without a grade penalty before a specific deadline, usually a few weeks into the term. If you decide to withdraw from the capstone before the start of the Fall 2025 term, you will not receive a 'W' notation on your transcript, assuming the withdrawal occurs within the designated withdrawal period set by the university.\\n\\nHowever, it's important to note that:\\n\\n1. **Check the Deadline**: Always verify the exact withdrawal deadline for the Fall 2025 term from the university's official academic calendar or with the registrar's office. Withdrawals after this deadline may result in a 'W' notation or other penalties.\\n\\n2. **Impact on Graduation**: Withdrawing from the capstone may affect your ability to graduate on time, as you would need to replace the capstone credits with other required coursework.\\n\\n3. **Consult with Advisors**: It's advisable to discuss your decision with your academic advisor or the capstone coordinator to understand the full implications and to explore alternative options to meet your major requirements.\\n\\n4. **Policy Variations**: Policies can change, so it's crucial to refer to the most current academic policies provided by Fulbright University for the most accurate and up-to-date information.\\n\\nIn summary, if you withdraw from the capstone before the established withdrawal deadline for the Fall 2025 term, there should be no 'W' notation on your transcript, but you should consult with the appropriate university officials to fully understand the implications of your decision.\"}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Llama 3.1 Tulu 8B is: \n",
      "{'role': 'assistant', 'content': \"Yes, you are eligible to take Financial Economics in Spring 2024. Generally, course prerequisites are designed to ensure that students have the necessary foundational knowledge to succeed in more advanced courses. Having completed both Microeconomic Analysis and Macroeconomic Analysis, you should have a strong understanding of economic principles, which are essential for studying Financial Economics. This course typically builds upon the micro and macroeconomic foundations to explore the functioning of financial markets, the behavior of financial institutions, and the role of financial markets in the economy. Always check the specific course prerequisites listed in the university's course catalog or consult with an academic advisor for the most accurate and up-to-date information.\"}\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T00:16:05.145718Z",
     "start_time": "2025-03-21T23:51:43.146845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Qwene 7B-1M\n",
    "for question in prompts_3:\n",
    "    questions = []\n",
    "    questions.append(system_role)\n",
    "    questions.append(question)\n",
    "    outputs = pipeline_llama_31_tulu_8B(questions, max_new_tokens = 1024, num_beams = 2)\n",
    "    print(\"The answer by Llama 3.1 Tulu 8B is: \")\n",
    "    print(outputs[0]['generated_text'][-1])\n",
    "    print(\"-\"*100)"
   ],
   "id": "43c52f6ce8c90418",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer by Llama 3.1 Tulu 8B is: \n",
      "{'role': 'assistant', 'content': 'No, withdrawal from a course after the 4th week does not result in a 4 credits deduction in the total credits covered by financial aid. When a student withdraws from a course after the drop/add period, the course remains on the transcript with a \"W\" grade, which does not affect the GPA. Financial aid calculations typically consider the student\\'s enrollment status (full-time, part-time, etc.) based on the number of credits attempted, not the number of credits successfully completed or grades received. Therefore, withdrawing from a course does not directly impact the total credits covered by financial aid. However, it may affect the student\\'s overall credit load for the term, which could impact eligibility for certain types of financial aid. Students should consult with the Financial Aid Office for specific details on how course withdrawals might affect their financial aid package.'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Llama 3.1 Tulu 8B is: \n",
      "{'role': 'assistant', 'content': \"Answer: If you withdraw from Capstone II, you will need to complete the 8 credits of coursework as specified in your major's requirements, as the capstone pathway has been discontinued. It's important to consult with your academic advisor to understand the specific courses you need to take to fulfill these requirements and to ensure you remain on track to graduate. Keep in mind that withdrawing from a capstone may also affect your academic standing and could potentially impact your eligibility for Graduation with Honors if you were previously on track to receive this distinction. Always check the academic policies of Fulbright University for the most current information, as policies may change.\"}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Llama 3.1 Tulu 8B is: \n",
      "{'role': 'assistant', 'content': \"The Fulbright Program, which is administered in Vietnam by the Ministry of Education and Training (MOET), adheres to MOET's regulations regarding study duration. According to these regulations, the maximum time to complete a program cannot exceed double the standard duration of the program. For most undergraduate and graduate programs, the standard duration is 4 years. Therefore, under MOET regulations, you would have a maximum of 8 years to complete your studies with financial aid from the Fulbright Program.\\n\\nIt is important to note that while the regulations allow for up to 8 years to complete a program, students are typically expected to complete their studies within the standard duration, which is 4 years for most programs. Extensions beyond the standard duration may be considered on a case-by-case basis and are subject to approval by MOET and the Fulbright Commission.\\n\\nIn summary, yes, you can continue your study for more than 4 years with financial aid from the Fulbright Program, but only up to a maximum of 8 years, as per MOET regulations. However, it is advisable to discuss your specific situation with the Fulbright Commission and your academic institution to ensure compliance with all requirements and to understand the implications of extending your study period.\"}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Llama 3.1 Tulu 8B is: \n",
      "{'role': 'assistant', 'content': 'Certainly! In the context of academic policy regarding double majors, cross-listed courses and elective applied courses have distinct characteristics:\\n\\n**Cross-listed Courses:**\\n- **Definition:** Cross-listed courses are those that are offered by two or more departments or programs. They carry the same course number and content, but may have different course codes (e.g., MATH 201 for Mathematics and STAT 201 for Statistics).\\n- **Usage in Double Majors:** Up to 8 credits (equivalent to two courses) from cross-listed courses can be double-counted towards the requirements of two different majors (or a major and a minor). This means if a course like MATH 201/STAT 201 is cross-listed, it can count towards both a Mathematics major and a Statistics minor, for example, without exceeding the 8-credit limit.\\n\\n**Elective Applied Courses:**\\n- **Definition:** Elective applied courses are those that are not necessarily cross-listed but are chosen by the student to fulfill the applied mathematics requirements of their major. These courses are typically chosen from a list of approved courses that emphasize the application of mathematical principles in various fields (e.g., Engineering, Computer Science, Biology, etc.).\\n- **Usage in Double Majors:** Elective applied courses chosen to satisfy the applied mathematics requirements of a major can also be considered for double-counting, but they are not governed by the same 8-credit limit as cross-listed courses. Instead, the decision to double-count these courses towards another major or minor is at the discretion of the academic departments involved, and they must be approved by the relevant academic advisors. There is no automatic limit like the 8-credit rule for cross-listed courses, but departments may set their own limits or guidelines.\\n\\n**Example Scenario:**\\n- Suppose a student is pursuing a double major in Mathematics and Computer Science. They take MATH 201/STAT 201 as a cross-listed course, which counts for 8 credits towards both majors (satisfying the cross-listed course limit).\\n- Additionally, they take an elective applied course in Computer Science (e.g., COMP 250: Introduction to Algorithms), which is not cross-listed with Mathematics but is approved as an applied mathematics course for their Mathematics major.\\n- In this scenario, the cross-listed course MATH 201/STAT 201 uses up the 8-credit limit for double-counting. The COMP 250 course does not fall under this limit but would need to be approved by both departments for double-counting.\\n\\nIn summary, cross-listed courses have a specific 8-credit limit for double-counting, while elective applied courses do not have an automatic limit but require departmental approval for double-counting.'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Llama 3.1 Tulu 8B is: \n",
      "{'role': 'assistant', 'content': \"The capstone withdrawal policy at Fulbright University allows students to withdraw from a capstone course before the halfway point of the term without receiving an 'F' on their GPA. The halfway point for an 8-week course is the end of week 4, and for a full-term course, it is the end of week 8. If you choose to withdraw from the capstone before these deadlines, you will not receive a grade that affects your GPA; instead, a 'W' (withdrawal) notation will be recorded on your transcript for the course from which you withdraw. This 'W' notation does not impact your GPA but does indicate that you withdrew from the course.\\n\\nTo formally withdraw from the capstone course, you must submit a Course Withdrawal form to the Registrar’s Office before the withdrawal deadline for the term. It is important to note that attempting to withdraw after the halfway point will result in an 'F' grade, which will affect your GPA.\\n\\nFor the Fall 2025 term specifically, the exact withdrawal deadlines will be communicated by the Registrar’s Office closer to the start of the term. Make sure to adhere to these deadlines to avoid any negative impact on your academic record.\\n\\nIn summary, if you withdraw from the capstone before the halfway point of the term, you will receive a 'W' notation on your transcript, which does not affect your GPA. However, if you withdraw after the halfway point, you will receive an 'F' grade, which will affect your GPA. Always check with the Registrar’s Office for the specific deadlines applicable to the Fall 2025 term.\"}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The answer by Llama 3.1 Tulu 8B is: \n",
      "{'role': 'assistant', 'content': \"Yes, you are eligible to take Financial Economics in Spring 2024. Generally, after completing the foundational courses in Microeconomic Analysis and Macroeconomic Analysis, students are prepared to take more advanced courses such as Financial Economics. However, eligibility can also depend on specific program requirements and prerequisites set by the department or university. It's always a good idea to check the official course catalog or consult with your academic advisor to ensure you meet all the necessary criteria.\"}\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Question-answering",
   "id": "eeaaceb739507431"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T14:51:14.517192Z",
     "start_time": "2025-03-21T14:51:11.690986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for question in prompts:\n",
    "    print(question['content'])\n",
    "    context = \"\"\"\n",
    "    There are two Withdrawal periods per term. The first period starts after 4:00 PM Friday, Week 2 to 4:00 PM Friday, Week 4. During this time, students can request to withdraw from the course without any consequence. The second period starts after 4:00 PM Friday, Week 4 to 4:00 PM Friday, Week 8. Attempted credits and a “W” grade will be recorded on the transcript for the course(s) from which students withdraw in this period. This grade does not affect a student’s GPA. In order to withdraw from a course, students need to submit the Course Withdrawal form to the Registrar’s Office, following the above timeline. A student who withdraws from a course beyond the halfway point of that course (end of week 4 for an 8-week course or end of week 8 for a full-term course) automatically receives a grade of ‘F’, which will affect the student’s GPA. Exceptions will only be permitted in exceptional circumstances (see Section 12), and it requires supporting documentation as well as the authorization of course instructor(s), the student’s academic/major advisor, and the Registrar’s Office. In the case of approved exceptional circumstances, tuition credit will be considered and follow the “Fees & Charges Guide” for undergraduate students. No reduction in tuition is granted to students whose course withdrawal results in fewer than 12 academic credits. Students should be aware that course withdrawals may affect their financial aid eligibility if resulting in fewer than the minimum credits for full-time enrollment.\n",
    "    \"\"\"\n",
    "    outputs = pipeline_qwen_25_72B_QA(question = \"Does withdrawal from a course after 4th week result in a 4 credits deduction in the total credits covered by financial aid?\", context = context, doc_stride = 370, max_answer_len = 370)\n",
    "    print(outputs)"
   ],
   "id": "c96176f4962f2c83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does withdrawal from a course after 4th week result in a 4 credits deduction in the total credits covered by financial aid?\n",
      "{'score': 0.22592197358608246, 'start': 1359, 'end': 1522, 'answer': 'Students should be aware that course withdrawals may affect their financial aid eligibility if resulting in fewer than the minimum credits for full-time enrollment'}\n",
      "What are the consequences if I withdraw from capstone II?\n",
      "{'score': 0.22592197358608246, 'start': 1359, 'end': 1522, 'answer': 'Students should be aware that course withdrawals may affect their financial aid eligibility if resulting in fewer than the minimum credits for full-time enrollment'}\n",
      "Can I continue my study for more than 4 years with financial aid?\n",
      "{'score': 0.22592197358608246, 'start': 1359, 'end': 1522, 'answer': 'Students should be aware that course withdrawals may affect their financial aid eligibility if resulting in fewer than the minimum credits for full-time enrollment'}\n",
      "Can you differentiate a cross-listed course and elective applied course in the case of a double major?\n",
      "{'score': 0.22592197358608246, 'start': 1359, 'end': 1522, 'answer': 'Students should be aware that course withdrawals may affect their financial aid eligibility if resulting in fewer than the minimum credits for full-time enrollment'}\n",
      "I would like to ask for the capstone withdrawal policy. What will be the impact when I choose to drop the capstone before the Fall 2025 term starts?\n",
      "{'score': 0.22592197358608246, 'start': 1359, 'end': 1522, 'answer': 'Students should be aware that course withdrawals may affect their financial aid eligibility if resulting in fewer than the minimum credits for full-time enrollment'}\n",
      "Can you differentiate a cross-listed course and elective applied course in the case of a double major?\n",
      "{'score': 0.22592197358608246, 'start': 1359, 'end': 1522, 'answer': 'Students should be aware that course withdrawals may affect their financial aid eligibility if resulting in fewer than the minimum credits for full-time enrollment'}\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Llama 3.3 70B",
   "id": "5863c3bdc2d7c76a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model loading",
   "id": "4a3db3cd29c80e59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T01:11:10.092389Z",
     "start_time": "2025-03-22T01:11:10.085507Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.empty_cache()",
   "id": "9e3d0f048e9c3177",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T01:14:18.646272Z",
     "start_time": "2025-03-22T01:14:13.459572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_llama_33_70B = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "pipeline_llama_33_70B = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_llama_33_70B,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")"
   ],
   "id": "810f43988826622d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e562ef9621764acfa7d2c681a7c2723e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T01:14:41.598992Z",
     "start_time": "2025-03-22T01:14:38.171316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Qwene 7B-1M\n",
    "for question in prompts_1:\n",
    "    questions = []\n",
    "    questions.append(system_role)\n",
    "    questions.append(question)\n",
    "    outputs = pipeline_llama_33_70B(questions, max_new_tokens = 1024, num_beams = 2)\n",
    "    print(\"The answer by Llama 3.3 70B is: \")\n",
    "    print(outputs[0]['generated_text'][-1])\n",
    "    print(\"-\"*100)"
   ],
   "id": "f5b3ae68a9c4a540",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m questions\u001B[38;5;241m.\u001B[39mappend(system_role)\n\u001B[1;32m      5\u001B[0m questions\u001B[38;5;241m.\u001B[39mappend(question)\n\u001B[0;32m----> 6\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline_llama_33_70B\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquestions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1024\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_beams\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe answer by Llama 3.3 70B is: \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(outputs[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgenerated_text\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:278\u001B[0m, in \u001B[0;36mTextGenerationPipeline.__call__\u001B[0;34m(self, text_inputs, **kwargs)\u001B[0m\n\u001B[1;32m    275\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(first_item, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m, \u001B[38;5;28mdict\u001B[39m)):\n\u001B[1;32m    276\u001B[0m     \u001B[38;5;66;03m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001B[39;00m\n\u001B[1;32m    277\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(first_item, \u001B[38;5;28mdict\u001B[39m):\n\u001B[0;32m--> 278\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mChat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext_inputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    279\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    280\u001B[0m         chats \u001B[38;5;241m=\u001B[39m (Chat(chat) \u001B[38;5;28;01mfor\u001B[39;00m chat \u001B[38;5;129;01min\u001B[39;00m text_inputs)  \u001B[38;5;66;03m# 🐈 🐈 🐈\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/transformers/pipelines/base.py:1366\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1358\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[1;32m   1359\u001B[0m         \u001B[38;5;28miter\u001B[39m(\n\u001B[1;32m   1360\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_iterator(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1363\u001B[0m         )\n\u001B[1;32m   1364\u001B[0m     )\n\u001B[1;32m   1365\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1366\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/transformers/pipelines/base.py:1373\u001B[0m, in \u001B[0;36mPipeline.run_single\u001B[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[0m\n\u001B[1;32m   1371\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mrun_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[1;32m   1372\u001B[0m     model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess(inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpreprocess_params)\n\u001B[0;32m-> 1373\u001B[0m     model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1374\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpostprocess(model_outputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpostprocess_params)\n\u001B[1;32m   1375\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/transformers/pipelines/base.py:1273\u001B[0m, in \u001B[0;36mPipeline.forward\u001B[0;34m(self, model_inputs, **forward_params)\u001B[0m\n\u001B[1;32m   1271\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[1;32m   1272\u001B[0m         model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_inputs, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m-> 1273\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1274\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_outputs, device\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m   1275\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:383\u001B[0m, in \u001B[0;36mTextGenerationPipeline._forward\u001B[0;34m(self, model_inputs, **generate_kwargs)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeneration_config\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m generate_kwargs:\n\u001B[1;32m    381\u001B[0m     generate_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeneration_config\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgeneration_config\n\u001B[0;32m--> 383\u001B[0m generated_sequence \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mgenerate_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    384\u001B[0m out_b \u001B[38;5;241m=\u001B[39m generated_sequence\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    385\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/transformers/generation/utils.py:2250\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m   2242\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[1;32m   2243\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m   2244\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_beams,\n\u001B[1;32m   2245\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[1;32m   2246\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[1;32m   2247\u001B[0m     )\n\u001B[1;32m   2249\u001B[0m     \u001B[38;5;66;03m# 13. run beam sample\u001B[39;00m\n\u001B[0;32m-> 2250\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_beam_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2251\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2252\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeam_scorer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2253\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2254\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2255\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2256\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2257\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2258\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2260\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;241m==\u001B[39m GenerationMode\u001B[38;5;241m.\u001B[39mGROUP_BEAM_SEARCH:\n\u001B[1;32m   2261\u001B[0m     \u001B[38;5;66;03m# 11. prepare beam search scorer\u001B[39;00m\n\u001B[1;32m   2262\u001B[0m     beam_scorer \u001B[38;5;241m=\u001B[39m BeamSearchScorer(\n\u001B[1;32m   2263\u001B[0m         batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[1;32m   2264\u001B[0m         num_beams\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_beams,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2270\u001B[0m         max_length\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mmax_length,\n\u001B[1;32m   2271\u001B[0m     )\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/transformers/generation/utils.py:3452\u001B[0m, in \u001B[0;36mGenerationMixin._beam_search\u001B[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001B[0m\n\u001B[1;32m   3449\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m stack_model_outputs(outputs_per_sub_batch, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget_text_config())\n\u001B[1;32m   3451\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# Unchanged original behavior\u001B[39;00m\n\u001B[0;32m-> 3452\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m   3454\u001B[0m \u001B[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001B[39;00m\n\u001B[1;32m   3455\u001B[0m model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_model_kwargs_for_generation(\n\u001B[1;32m   3456\u001B[0m     outputs,\n\u001B[1;32m   3457\u001B[0m     model_kwargs,\n\u001B[1;32m   3458\u001B[0m     is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[1;32m   3459\u001B[0m )\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/accelerate/hooks.py:176\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    174\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 176\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action\u001B[38;5;241m.\u001B[39mNOTIFY, Action\u001B[38;5;241m.\u001B[39mNOTIFY_ALWAYS) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m--> 172\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:839\u001B[0m, in \u001B[0;36mLlamaForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001B[0m\n\u001B[1;32m    836\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m    838\u001B[0m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[0;32m--> 839\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    840\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    841\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    842\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    843\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    844\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    845\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    846\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    847\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    848\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    849\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    850\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    851\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    853\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    854\u001B[0m \u001B[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:594\u001B[0m, in \u001B[0;36mLlamaModel.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001B[0m\n\u001B[1;32m    582\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m    583\u001B[0m         decoder_layer\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[1;32m    584\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    591\u001B[0m         position_embeddings,\n\u001B[1;32m    592\u001B[0m     )\n\u001B[1;32m    593\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 594\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    595\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    596\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcausal_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    597\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    598\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    599\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    600\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    601\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    602\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    603\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mflash_attn_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    604\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    606\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    608\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/accelerate/hooks.py:176\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    174\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 176\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:352\u001B[0m, in \u001B[0;36mLlamaDecoderLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001B[0m\n\u001B[1;32m    350\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[1;32m    351\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpost_attention_layernorm(hidden_states)\n\u001B[0;32m--> 352\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    353\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m hidden_states\n\u001B[1;32m    355\u001B[0m outputs \u001B[38;5;241m=\u001B[39m (hidden_states,)\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/accelerate/hooks.py:176\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    174\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 176\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:190\u001B[0m, in \u001B[0;36mLlamaMLP.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    189\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m--> 190\u001B[0m     down_proj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdown_proj(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mact_fn(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgate_proj(x)) \u001B[38;5;241m*\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mup_proj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    191\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m down_proj\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/accelerate/hooks.py:176\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    174\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 176\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T02:42:54.267682Z",
     "start_time": "2025-03-22T01:14:42.942668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Qwene 7B-1M\n",
    "for question in prompts_2:\n",
    "    questions = []\n",
    "    questions.append(system_role)\n",
    "    questions.append(question)\n",
    "    outputs = pipeline_llama_33_70B(questions, max_new_tokens = 1024, num_beams = 2)\n",
    "    print(\"The answer by Llama 3.3 70B is: \")\n",
    "    print(outputs[0]['generated_text'][-1])\n",
    "    print(\"-\"*100)"
   ],
   "id": "9b9cf02b6a386e7d",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m questions\u001B[38;5;241m.\u001B[39mappend(system_role)\n\u001B[1;32m      5\u001B[0m questions\u001B[38;5;241m.\u001B[39mappend(question)\n\u001B[0;32m----> 6\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline_llama_33_70B\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquestions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1024\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_beams\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe answer by Llama 3.3 70B is: \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(outputs[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgenerated_text\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:278\u001B[0m, in \u001B[0;36mTextGenerationPipeline.__call__\u001B[0;34m(self, text_inputs, **kwargs)\u001B[0m\n\u001B[1;32m    275\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(first_item, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m, \u001B[38;5;28mdict\u001B[39m)):\n\u001B[1;32m    276\u001B[0m     \u001B[38;5;66;03m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001B[39;00m\n\u001B[1;32m    277\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(first_item, \u001B[38;5;28mdict\u001B[39m):\n\u001B[0;32m--> 278\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mChat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext_inputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    279\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    280\u001B[0m         chats \u001B[38;5;241m=\u001B[39m (Chat(chat) \u001B[38;5;28;01mfor\u001B[39;00m chat \u001B[38;5;129;01min\u001B[39;00m text_inputs)  \u001B[38;5;66;03m# 🐈 🐈 🐈\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/transformers/pipelines/base.py:1366\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1358\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[1;32m   1359\u001B[0m         \u001B[38;5;28miter\u001B[39m(\n\u001B[1;32m   1360\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_iterator(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1363\u001B[0m         )\n\u001B[1;32m   1364\u001B[0m     )\n\u001B[1;32m   1365\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1366\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/transformers/pipelines/base.py:1373\u001B[0m, in \u001B[0;36mPipeline.run_single\u001B[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[0m\n\u001B[1;32m   1371\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mrun_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[1;32m   1372\u001B[0m     model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess(inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpreprocess_params)\n\u001B[0;32m-> 1373\u001B[0m     model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1374\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpostprocess(model_outputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpostprocess_params)\n\u001B[1;32m   1375\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/transformers/pipelines/base.py:1273\u001B[0m, in \u001B[0;36mPipeline.forward\u001B[0;34m(self, model_inputs, **forward_params)\u001B[0m\n\u001B[1;32m   1271\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[1;32m   1272\u001B[0m         model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_inputs, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m-> 1273\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1274\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_outputs, device\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m   1275\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:383\u001B[0m, in \u001B[0;36mTextGenerationPipeline._forward\u001B[0;34m(self, model_inputs, **generate_kwargs)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeneration_config\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m generate_kwargs:\n\u001B[1;32m    381\u001B[0m     generate_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeneration_config\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgeneration_config\n\u001B[0;32m--> 383\u001B[0m generated_sequence \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mgenerate_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    384\u001B[0m out_b \u001B[38;5;241m=\u001B[39m generated_sequence\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    385\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/transformers/generation/utils.py:2250\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m   2242\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[1;32m   2243\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m   2244\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_beams,\n\u001B[1;32m   2245\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[1;32m   2246\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[1;32m   2247\u001B[0m     )\n\u001B[1;32m   2249\u001B[0m     \u001B[38;5;66;03m# 13. run beam sample\u001B[39;00m\n\u001B[0;32m-> 2250\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_beam_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2251\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2252\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeam_scorer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2253\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2254\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2255\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2256\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2257\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2258\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2260\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;241m==\u001B[39m GenerationMode\u001B[38;5;241m.\u001B[39mGROUP_BEAM_SEARCH:\n\u001B[1;32m   2261\u001B[0m     \u001B[38;5;66;03m# 11. prepare beam search scorer\u001B[39;00m\n\u001B[1;32m   2262\u001B[0m     beam_scorer \u001B[38;5;241m=\u001B[39m BeamSearchScorer(\n\u001B[1;32m   2263\u001B[0m         batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[1;32m   2264\u001B[0m         num_beams\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_beams,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2270\u001B[0m         max_length\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mmax_length,\n\u001B[1;32m   2271\u001B[0m     )\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/transformers/generation/utils.py:3452\u001B[0m, in \u001B[0;36mGenerationMixin._beam_search\u001B[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001B[0m\n\u001B[1;32m   3449\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m stack_model_outputs(outputs_per_sub_batch, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget_text_config())\n\u001B[1;32m   3451\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# Unchanged original behavior\u001B[39;00m\n\u001B[0;32m-> 3452\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m   3454\u001B[0m \u001B[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001B[39;00m\n\u001B[1;32m   3455\u001B[0m model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_model_kwargs_for_generation(\n\u001B[1;32m   3456\u001B[0m     outputs,\n\u001B[1;32m   3457\u001B[0m     model_kwargs,\n\u001B[1;32m   3458\u001B[0m     is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[1;32m   3459\u001B[0m )\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/accelerate/hooks.py:176\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    174\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 176\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action\u001B[38;5;241m.\u001B[39mNOTIFY, Action\u001B[38;5;241m.\u001B[39mNOTIFY_ALWAYS) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m--> 172\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:839\u001B[0m, in \u001B[0;36mLlamaForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001B[0m\n\u001B[1;32m    836\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m    838\u001B[0m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[0;32m--> 839\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    840\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    841\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    842\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    843\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    844\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    845\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    846\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    847\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    848\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    849\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    850\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    851\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    853\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    854\u001B[0m \u001B[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:594\u001B[0m, in \u001B[0;36mLlamaModel.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001B[0m\n\u001B[1;32m    582\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m    583\u001B[0m         decoder_layer\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[1;32m    584\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    591\u001B[0m         position_embeddings,\n\u001B[1;32m    592\u001B[0m     )\n\u001B[1;32m    593\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 594\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    595\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    596\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcausal_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    597\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    598\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    599\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    600\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    601\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    602\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    603\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mflash_attn_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    604\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    606\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    608\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/accelerate/hooks.py:176\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    174\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 176\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:336\u001B[0m, in \u001B[0;36mLlamaDecoderLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001B[0m\n\u001B[1;32m    333\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_layernorm(hidden_states)\n\u001B[1;32m    335\u001B[0m \u001B[38;5;66;03m# Self Attention\u001B[39;00m\n\u001B[0;32m--> 336\u001B[0m hidden_states, self_attn_weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_attn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    337\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    338\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    339\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    340\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    341\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    342\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    343\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    344\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    345\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    346\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    347\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m hidden_states\n\u001B[1;32m    349\u001B[0m \u001B[38;5;66;03m# Fully Connected\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/accelerate/hooks.py:176\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    174\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 176\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:270\u001B[0m, in \u001B[0;36mLlamaAttention.forward\u001B[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001B[0m\n\u001B[1;32m    267\u001B[0m input_shape \u001B[38;5;241m=\u001B[39m hidden_states\u001B[38;5;241m.\u001B[39mshape[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    268\u001B[0m hidden_shape \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m*\u001B[39minput_shape, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead_dim)\n\u001B[0;32m--> 270\u001B[0m query_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mq_proj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mview(hidden_shape)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    271\u001B[0m key_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mk_proj(hidden_states)\u001B[38;5;241m.\u001B[39mview(hidden_shape)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    272\u001B[0m value_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mv_proj(hidden_states)\u001B[38;5;241m.\u001B[39mview(hidden_shape)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/accelerate/hooks.py:176\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    174\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 176\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/.conda/envs/nlp_project/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Qwene 7B-1M\n",
    "for question in prompts_2:\n",
    "    questions = []\n",
    "    questions.append(system_role)\n",
    "    questions.append(question)\n",
    "    outputs = pipeline_llama_33_70B(questions, max_new_tokens = 1024, num_beams = 2)\n",
    "    print(\"The answer by Llama 3.3 70B is: \")\n",
    "    print(outputs[0]['generated_text'][-1])\n",
    "    print(\"-\"*100)"
   ],
   "id": "3fddd8e0c3b43c66"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
